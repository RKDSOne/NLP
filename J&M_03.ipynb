{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** HMM Formalism ($HMM\\space\\lambda = (A,B)$) ** (cf. J&M ch5.5:23)\n",
    "\n",
    "    * $Q = q_1...q_N$, a set of $N$ *states*.\n",
    "    * $A = a_{11}a_{12}...a_{nn}$, where $\\sum_{j=1}^na_{ij} = 1, \\forall i$, a *transition probability matrix* $A$.\n",
    "    * $O = o_1...o_T$, a sequence of *observations*.\n",
    "    * $B = b_i(o_t)$, a sequence of *emission probabilities* (i.e. the probability of an observation $o_t$ being omitted at state $i$.\n",
    "    * $q_0,q_F$, *start state* and *end state*.\n",
    "\n",
    "\n",
    "* ** Framing Problems ** (cf. J&M ch6.2:7)\n",
    "\n",
    "    * ** Problem 1 (Computing Likelihood): ** Given an HMM $\\lambda=(A,B)$ and an observation sequence $O$, determine the likelihood $P(O|\\lambda)$.\n",
    "\n",
    "    * ** Problem 2 (Decoding): ** Given an observation sequence $O$ and an HMM $\\lambda=(A,B)$, discover the best hidden state sequence $Q$.\n",
    "\n",
    "    * ** Problem 3 (Learning): ** Given an observation sequence $O$ and the set of states in the HMM, learn the HMM parameters $A$ and $B$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Computing Likelihood\n",
    "\n",
    "**NB: The following version of forward pass is non-generalizable, written out in syntactically transparent version for demonstration purpose. A serious version will come up in section C. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ICE CREAM MODEL\n",
    " \n",
    "#  -.8-> HOT -.7-> HOT -.3-> COLD\n",
    "#         |         |         |\n",
    "#        .4        .2        .1\n",
    "#         3         1         3\n",
    "\n",
    "# Question: P(O = [3,1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Math ** (cf. J&M ch6.3:9,eq.6.10,12)\n",
    "\n",
    "    * $P(O,Q) = P(O|Q)\\cdot P(Q) = \\prod_{i=1}^nP(o_i|q_i)\\times\\prod_{i=1}^nP(q_i|q_{i-1})$, where $n$ is the ordered index sequence of observations.\n",
    "    \n",
    "    * $P(O) = \\sum_QP(O,Q) = \\sum_QP(O|Q)\\cdot P(Q)$\n",
    "    \n",
    "\n",
    "* ** Computation ** (cf. ibid.:10-12)\n",
    "\n",
    "    * Brute-Force: $O(N^T)$, where $N$ is the set of tags, $T$ the length of observation sequence.\n",
    "    \n",
    "    * Forward Algorithm: $O(N^2T)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TOY HMM\n",
    "class LMD:\n",
    "    def __init__(self):\n",
    "        self.Q = ['START','COLD','HOT'] # NB: the order matters for indexing reason later.\n",
    "        self.A = {('START','START'):0.,('START','HOT'):.8,('START','COLD'):.2,\n",
    "                  ('HOT','START'):0.,('HOT','HOT'):.7,('HOT','COLD'):.3,\n",
    "                  ('COLD','START'):0.,('COLD','HOT'):.4,('COLD','COLD'):.6}\n",
    "        self.B = {('HOT',1):.2,('HOT',2):.4,('HOT',3):.4,\n",
    "                  ('COLD',1):.5,('COLD',2):.4,('COLD',3):.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute-Force \"Baseline\"\n",
    "\n",
    "* num_states | processing time:\n",
    "    * 3 | 300 $\\mu$s\n",
    "    * 15 | 800 ms\n",
    "    * 20 | 30 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from nltk.util import ngrams\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brute_force(lmd,O,num_states=3):\n",
    "    Q, A, B = lmd.Q, lmd.A, lmd.B\n",
    "    Qs = [list(Q) for Q in product(Q[1:],repeat=num_states)] # all possible t-state sequences.\n",
    "    # computation components\n",
    "    def Pr_Q(Q): # Q = [q_1,...,q_n].\n",
    "        Q = deepcopy(Q)\n",
    "        Q.insert(0,'START')\n",
    "        p = 1.\n",
    "        for q_trans in ngrams(Q,2): # q_trans = (q_i-1,q_i).\n",
    "            p *= A[q_trans]\n",
    "        return p\n",
    "    def Pr_O_given_Q(O,Q): # O = [o_1,...,o_t], Q = [q_1,...,q_n].\n",
    "        return reduce(mul,map(B.get,zip(Q,O)))\n",
    "    def Pr_O(O):\n",
    "        return sum(Pr_O_given_Q(O,Q)*Pr_Q(Q) for Q in Qs)      \n",
    "    print Pr_O(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026264\n",
      "CPU times: user 31.8 s, sys: 230 ms, total: 32 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lmd = LMD()\n",
    "O = [3,1,3]\n",
    "brute_force(lmd,O,num_states=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Algorithm\n",
    "\n",
    "* num_states | processing time:\n",
    "    * 3 | 300 $\\mu$s\n",
    "    * 15 | 400 $\\mu$s\n",
    "    * 20 | 500 $\\mu$s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(lmd,O):\n",
    "    Q, A, B = lmd.Q, lmd.A, lmd.B\n",
    "    N, T = len(Q), len(O)\n",
    "    fwd = np.zeros((N,T))\n",
    "    for s in xrange(1,N):\n",
    "        fwd[s][1] = A[('START',Q[s])] * B[(Q[s],O[1])]\n",
    "            # initialize: START -> all states\n",
    "    for t in xrange(2,T):\n",
    "        for s in xrange(1,N):\n",
    "            fwd[s][t] = sum(fwd[s_prime][t-1]*A[(Q[s_prime],Q[s])]*B[(Q[s],O[t])]\n",
    "                            for s_prime in xrange(1,N)) \n",
    "            # forward passing: sum ( each prev state * prev->current * current->omission )\n",
    "    print 'Pr(O) = ', sum(fwd[:,-1])\n",
    "    \n",
    "# fwd matrix in when O = [3,1,3], (cf. J&M ch6.4:10,fig.6.7)\n",
    "# [[ 0.        0.        0.        0.      ]\n",
    "#  [ 0.        0.02      0.054     0.004632]\n",
    "#  [ 0.        0.32      0.0464    0.021632]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(O) =  0.026264\n",
      "CPU times: user 271 µs, sys: 94 µs, total: 365 µs\n",
      "Wall time: 293 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O = ['<s>',3,1,3]\n",
    "forward(LMD(),O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Decoding: Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TOY HMM again\n",
    "class LMD:\n",
    "    def __init__(self):\n",
    "        self.Q = ['START','COLD','HOT'] # NB: the order matters for indexing reason later.\n",
    "        self.A = {('START','START'):0.,('START','HOT'):.8,('START','COLD'):.2,\n",
    "                  ('HOT','START'):0.,('HOT','HOT'):.7,('HOT','COLD'):.3,\n",
    "                  ('COLD','START'):0.,('COLD','HOT'):.4,('COLD','COLD'):.6}\n",
    "        self.B = {('HOT',1):.2,('HOT',2):.4,('HOT',3):.4,\n",
    "                  ('COLD',1):.5,('COLD',2):.4,('COLD',3):.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Math (Recursion Step) **\n",
    "\n",
    "    * $viterbi_{s,t} = max(viterbi_{s',t-1} * A_{s',s} * B_{s,t})$, i.e. at each time $t$, at state $s$, set viterbi as **the largest value** of the multiplication $\\mathtt{value\\,of\\,prev\\,state}\\times\\mathtt{transition\\,from\\,prev\\,to\\,current}\\times\\mathtt{current\\,emission}$.\n",
    "    \n",
    "    * $backpointer_{s,t} = argmax(viterbi_{s',t-1} * A_{s',s})$, i.e. at each time $t$, at state $s$, set backpointer as the **index of largest value** of the multiplication $\\mathtt{value\\,of\\,prev\\,state}\\times\\mathtt{transition\\,from\\,prev\\,to\\,current}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(lmd,O):\n",
    "    Q, A, B = lmd.Q, lmd.A, lmd.B\n",
    "    N, T = len(Q), len(O)\n",
    "    viterbi, backpointer = np.zeros((N,T)), np.zeros((N,T),dtype=int)\n",
    "    for s in xrange(1,N): \n",
    "        # this is written in a more transparent syntax, for simplification, see J&M_02.\n",
    "        viterbi[s][1] = A[('START',Q[s])] * B[(Q[s],O[1])]\n",
    "        backpointer[s][1] = 0\n",
    "    for t in xrange(2,T):\n",
    "        for s in xrange(1,N):\n",
    "            viterbi[s][t] = max(viterbi[s_prime][t-1]*A[(Q[s_prime],Q[s])]*B[(Q[s],O[t])]\n",
    "                                for s_prime in xrange(1,N))\n",
    "            backpointer[s][t] = np.argmax([viterbi[s_prime][t-1]*A[(Q[s_prime],Q[s])]\n",
    "                                           for s_prime in xrange(1,N)])+1\n",
    "    max_state = np.argmax(viterbi[:,-1])\n",
    "    best_state_seq = []\n",
    "    for t in reversed(xrange(0,T)): \n",
    "        best_state_seq.insert(0,Q[max_state])\n",
    "        max_state = backpointer[max_state][t]\n",
    "    \n",
    "    print 'viterbi'\n",
    "    print viterbi\n",
    "    print 'backpointer'\n",
    "    print backpointer\n",
    "    print best_state_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbi\n",
      "[[ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.02      0.048     0.00288 ]\n",
      " [ 0.        0.32      0.0448    0.012544]]\n",
      "backpointer\n",
      "[[0 0 0 0]\n",
      " [0 0 2 1]\n",
      " [0 0 2 2]]\n",
      "['START', 'HOT', 'HOT', 'HOT']\n",
      "CPU times: user 1.48 ms, sys: 531 µs, total: 2.01 ms\n",
      "Wall time: 1.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O = ['<s>',3,1,3]\n",
    "viterbi(LMD(),O) \n",
    "    # NB: J&M ch6.4:13,fig.6.10 is not showing the best tag sequence for O=[3,1,3],\n",
    "    #  specifically, the figure is inconsistent with the transition/emission probabilities\n",
    "    #  the example is working with (e.g. the numbers in LMD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trigram HMM + Deleted Interpolation (Brown Corpus)\n",
    "\n",
    "**Comments: Somewhat disapointing, it doesn't outperform the previous bigram HMM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_brown(train_percentage=.8): # importing tagged brown.\n",
    "    from nltk.corpus import brown\n",
    "    from nltk.stem import PorterStemmer\n",
    "    print \"... loading sentences\"\n",
    "    brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "    print \"... stemming and lowercasing words\"\n",
    "    brown_tagged_sents = [[(PorterStemmer().stem(w).lower(),t) for w,t in tagged_sent]\n",
    "                   for tagged_sent in brown_tagged_sents]\n",
    "    print \"... padding sentences\"\n",
    "    brown_tagged_sents = [[('<s>','START')]+s+[('</s>','END')] for s in brown_tagged_sents]\n",
    "    cut_off = int(len(brown_tagged_sents)*.8)\n",
    "    \n",
    "    return (brown_tagged_sents[:cut_off], brown_tagged_sents[cut_off:]) # train,test: two lists of sents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading sentences\n",
      "... stemming and lowercasing words\n",
      "... padding sentences\n",
      "CPU times: user 24.1 s, sys: 500 ms, total: 24.6 s\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "brown_tagged_train, brown_tagged_test = load_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from numpy import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        print \"... setting up vocabularies for tags and words\"\n",
    "        self.size = sum(map(len,self.train))\n",
    "        self.vocab = list({w for sent in self.train for w,t in sent}) + ['unk'] # brown: len=31661, as T.\n",
    "        self.tagset = list({t for sent in self.train for w,t in sent}) + ['UNK'] # brown: len=452, as N.\n",
    "        self.btagset = Counter(ngrams([t for sent in self.train for w,t in sent],2)).keys()\n",
    "        self.w2i = Counter({w:i for i,w in enumerate(self.vocab)})\n",
    "        self.t2i = Counter({t:i for i,t in enumerate(self.tagset)})\n",
    "        self.bt2i = Counter({bt:i for i,bt in enumerate(self.btagset)})\n",
    "        print \"... building transition probability matrix\"\n",
    "        self.build_transition_matrix()\n",
    "        print \"... building emission probability matrix\"\n",
    "        self.build_emission_matrix()\n",
    "    \n",
    "    def build_transition_matrix(self):\n",
    "        # Comments: The following can be improved my cramping all\n",
    "        #  three ngram matrices into one, however, I suspect the \n",
    "        #  gigantic matrix resulted from that will cause some\n",
    "        #  computational difficulties.\n",
    "        \n",
    "        print \"    ... computing unigram probabilities\"\n",
    "        unigram_dict = Counter(t for sent in self.train for w,t in sent)\n",
    "        self.A_uni = np.ones(len(self.tagset))\n",
    "        for t in self.tagset:\n",
    "            count = unigram_dict[t]\n",
    "            self.A_uni[self.t2i[t]] = count if count!=0 else 1.\n",
    "        row_sum = sum(self.A_uni)\n",
    "        self.A_uni /= row_sum\n",
    "        self.A_uni[isnan(self.A_uni)] = 0.\n",
    "        \n",
    "        print \"    ... computing bigram transition probabilities\"\n",
    "        bi_trans_dict = Counter(ngrams([t for sent in self.train for w,t in sent],2))\n",
    "        self.A_bi = np.ones((len(self.tagset),len(self.tagset))) \n",
    "        for from_t in self.tagset:\n",
    "            for to_t in self.tagset:\n",
    "                trans_count = bi_trans_dict[(from_t,to_t)]\n",
    "                self.A_bi[self.t2i[from_t]][self.t2i[to_t]] = trans_count if trans_count!=0 else 1.\n",
    "        row_sums = np.apply_along_axis(sum, 1, self.A_bi)[:,np.newaxis]\n",
    "        self.A_bi /= row_sums\n",
    "        self.A_bi[isnan(self.A_bi)] = 0.\n",
    "        \n",
    "        print \"    ... computing trigram transition probabilities\"\n",
    "        tri_trans_dict = Counter(ngrams([t for sent in self.train for w,t in sent],3))\n",
    "        self.A_tri = np.ones((len(self.btagset),len(self.tagset)))\n",
    "        for from_t2,from_t1 in self.btagset: # i.e. t_{i-2}, t_{i-1}\n",
    "            for to_t in self.tagset:\n",
    "                trans_count = tri_trans_dict[(from_t2,from_t1,to_t)]\n",
    "                self.A_tri[self.bt2i[(from_t2,from_t1)]][self.t2i[to_t]] = trans_count if trans_count!=0 else 1.\n",
    "        row_sums = np.apply_along_axis(sum, 1, self.A_tri)[:,np.newaxis]\n",
    "        self.A_tri /= row_sums\n",
    "        self.A_tri[isnan(self.A_tri)] = 0.\n",
    "        \n",
    "        print \"    ... computing deleted interpolation coefficients\"\n",
    "        self.lmd1, self.lmd2, self.lmd3 = self.deleted_interpolation(unigram_dict,bi_trans_dict,tri_trans_dict)\n",
    "    \n",
    "    def build_emission_matrix(self):\n",
    "        print \"    ... counting emissions\"\n",
    "        emission_dict = Counter((w,t) for sent in self.train for w,t in sent)\n",
    "        print \"    ... computing emission probabilities\"\n",
    "        self.B = np.ones((len(self.tagset),len(self.vocab)))\n",
    "        for t in self.tagset:\n",
    "            for o in self.vocab:\n",
    "                emit_count = emission_dict[(o,t)]\n",
    "                self.B[self.t2i[t]][self.w2i[o]] = emit_count if emit_count!=0 else 1.\n",
    "        row_sums = np.apply_along_axis(sum, 1, self.B)[:,np.newaxis]\n",
    "        self.B /= row_sums\n",
    "        self.B[isnan(self.B)] = 0.\n",
    "    \n",
    "    def deleted_interpolation(self, unigrams, bigrams, trigrams):\n",
    "        # unigrams, bigrams, trigrams: 3 Counters.\n",
    "        def brants_divide(num,denom): # (cf. Brants(2000):226,fig.1)\n",
    "            return 0 if denom==0 else num/denom\n",
    "        lmds = {1:0, 2:0, 3:0}\n",
    "        for t1,t2,t3 in trigrams:\n",
    "            f123 = trigrams[(t1,t2,t3)] / bigrams[(t1,t2)]\n",
    "            lmds[np.argmax([-np.inf,\n",
    "                            brants_divide(trigrams[(t1,t2,t3)]-1,bigrams[(t1,t2)]-1),\n",
    "                            brants_divide(bigrams[(t2,t3)]-1,unigrams[t3]-1),\n",
    "                            brants_divide(unigrams[t3]-1,self.size-1)])] += f123\n",
    "        return np.array([lmds[1],lmds[2],lmds[3]]) / sum(np.array([lmds[1],lmds[2],lmds[3]]))\n",
    "\n",
    "    def viterbi(self, sent):\n",
    "        # sent: a sentence in the form, e.g. ['<s>',w1,...,wN,'</s>'].\n",
    "        \n",
    "        N, T = len(self.tagset), len(sent)\n",
    "        viterbi = np.zeros((N,T))\n",
    "        backpointer = np.zeros((N,T),dtype=int)\n",
    "        \n",
    "        for s in xrange(N):\n",
    "            viterbi[s][1] = self.A_bi[self.t2i['START']][s] * self.B[s][self.w2i[sent[1]]]\n",
    "            backpointer[s][1] = 0            \n",
    "            \n",
    "        for t in xrange(2,T):\n",
    "            for s in xrange(N):\n",
    "                prev_viterbi = []\n",
    "                prev_backpointer = []\n",
    "                for s_prime in xrange(N):\n",
    "                    max_trans = max(self.lmd1*self.A_uni[s] + \\\n",
    "                                    self.lmd2*self.A_bi[s_prime][s] + \\\n",
    "                                    self.lmd3*self.A_tri[self.bt2i[(self.tagset[s_prime_prime],\n",
    "                                                                    self.tagset[s_prime])]][s]\n",
    "                                    for s_prime_prime in xrange(N))\n",
    "                    prev_viterbi.append(viterbi[s_prime][t-1] * max_trans * self.B[s][self.w2i[sent[t]]])\n",
    "                    prev_backpointer.append(viterbi[s_prime][t-1] * max_trans)\n",
    "                viterbi[s][t] = max(prev_viterbi)\n",
    "                backpointer[s][t] = np.argmax(prev_backpointer)  \n",
    "                     \n",
    "        max_state = self.tagset.index('END') \n",
    "            # TEST: always give </s> END as tag.\n",
    "            #  otherwise: max_state = np.argmax(viterbi[:,T-1])\n",
    "        best_tagged_seq = []\n",
    "        for t in reversed(xrange(0,T)):\n",
    "            best_tagged_seq.insert(0,(sent[t],self.tagset[max_state]))\n",
    "            max_state = backpointer[max_state][t]\n",
    "        \n",
    "        return best_tagged_seq\n",
    "    \n",
    "    def tag(self, sent): # may use this for other passing purpose.\n",
    "        return self.viterbi(sent) \n",
    "\n",
    "    def evaluate(self, k=500, verbose_freq=100):\n",
    "        sample_idxs = random.sample(xrange(len(self.test)),k)\n",
    "        sample_test = [self.test[i] for i in sample_idxs] \n",
    "        accuracies = []\n",
    "        sent_lens = []\n",
    "        for i in xrange(k):\n",
    "            sent = [w if w in self.vocab else 'unk' for w,t in sample_test[i]]\n",
    "            tagged = self.tag(sent)\n",
    "            accuracy = sum(y==yhat for y,yhat in zip(sample_test[i][1:-1],tagged[1:-1])) / \\\n",
    "                                                                  len(tagged[1:-1])\n",
    "            if i!=0 and i%verbose_freq==0:\n",
    "                print \"    ... tagged %d sentences, current accuracy: %.2f%%\" % (i,np.mean(accuracies)*100) \n",
    "            accuracies += [accuracy]\n",
    "            sent_lens += [len(tagged[1:-1])] \n",
    "            \n",
    "        return [sent_lens, accuracies]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... setting up vocabularies for tags and words\n",
      "... building transition probability matrix\n",
      "    ... computing unigram probabilities\n",
      "    ... computing bigram transition probabilities\n",
      "    ... computing trigram transition probabilities\n",
      "    ... computing deleted interpolation coefficients\n",
      "... building emission probability matrix\n",
      "    ... counting emissions\n",
      "    ... computing emission probabilities\n",
      "CPU times: user 5.54 s, sys: 182 ms, total: 5.72 s\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hmm = HMM(brown_tagged_train,brown_tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ... tagged 100 sentences, current accuracy: 90.21%\n",
      "    ... tagged 200 sentences, current accuracy: 90.74%\n",
      "    ... tagged 300 sentences, current accuracy: 90.28%\n",
      "    ... tagged 400 sentences, current accuracy: 89.89%\n",
      "    ... tagged 500 sentences, current accuracy: 90.12%\n",
      "    ... tagged 600 sentences, current accuracy: 89.99%\n",
      "    ... tagged 700 sentences, current accuracy: 90.16%\n",
      "    ... tagged 800 sentences, current accuracy: 90.25%\n",
      "    ... tagged 900 sentences, current accuracy: 90.35%\n",
      "    ... tagged 1000 sentences, current accuracy: 90.16%\n",
      "    ... tagged 1100 sentences, current accuracy: 90.19%\n",
      "    ... tagged 1200 sentences, current accuracy: 90.22%\n",
      "    ... tagged 1300 sentences, current accuracy: 90.23%\n",
      "    ... tagged 1400 sentences, current accuracy: 90.28%\n",
      "    ... tagged 1500 sentences, current accuracy: 90.29%\n",
      "    ... tagged 1600 sentences, current accuracy: 90.24%\n",
      "    ... tagged 1700 sentences, current accuracy: 90.27%\n",
      "    ... tagged 1800 sentences, current accuracy: 90.26%\n",
      "    ... tagged 1900 sentences, current accuracy: 90.22%\n",
      "CPU times: user 4min 39s, sys: 1.19 s, total: 4min 40s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sent_lens, accuracies = hmm.evaluate(k=2000,verbose_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Learning: Forward-Backward\n",
    "\n",
    "**Tip:** To check forward and backward passings are implemented correctly, compute $P(O|\\lambda)$ on both, the results should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.set_printoptions(formatter={'float': '{: 0.15f}'.format})\n",
    "np.set_printoptions(suppress=True) # suppress pain-in-the-neck scientific-notation output in np matrices.\n",
    "def prt(num): print '{:.20f}'.format(num) # print a single number in non-scientific-notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TOY HMM\n",
    "LMD = {'Q':['PPSS','VB','TO','NN'], \n",
    "       'A': np.array([[.00013, .23,     .00079,   .0012],  # rows: from-states, cols: to-states.\n",
    "                      [.007,   .0038,   .035,     .047],   # rows=cols={PPSS,VB,TO,NN}, len=4\n",
    "                      [0.,     .83,     0.,       .00047], # e.g. A[2][1] = p(TO->VB) = .83.\n",
    "                      [.0045,  .004,    .016,     .087]]),\n",
    "       'B': np.array([[.37,    0.,     0.,     0.,  ],    # rows: emitting-state, cols: emitted-observations.\n",
    "                      [0.,     .0093,  0.,     .00012],   #  rows={PPSS,VB,TO,NN}, len=4\n",
    "                      [0.,     0.,     .99,    0.],       #  cols={i,want,to,race}, len=4  \n",
    "                      [0.,     .000054,0.,     .00057]]), # e.g. B[1][1] = p(VB->want) = .0093.\n",
    "       'from_start_probs': [.067, .019, .0043, .041],\n",
    "       'to_end_probs': [.001, .05, .001, .13]}\n",
    "\n",
    "O = ['i','want','to','race']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing Forward Probability $\\alpha_t(j)$ (state=j at time=t), given HMM $\\lambda=\\{A,B\\}$ and Observation $O$.\n",
    "\n",
    "**meaning:** The probability of ending up in state $j$ at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(observation=O,HMM=LMD): # (cf. J&M ch6.3:12,fig.6.9, with indexing from 0 instead)\n",
    "    Q, A, B = HMM['Q'], HMM['A'], HMM['B']\n",
    "    from_start = HMM['from_start_probs']\n",
    "    to_end = HMM['to_end_probs']\n",
    "    O = observation\n",
    "    N, T = len(Q), len(O)\n",
    "    fwd = np.zeros((N,T))\n",
    "    for s in xrange(N):\n",
    "        fwd[s][0] = from_start[s] * B[s][0]\n",
    "    for t in xrange(1,T):\n",
    "        for s in xrange(N):\n",
    "            fwd[s][t] = sum(fwd[s_prime][t-1] * A[s_prime][s] * B[s][t]\n",
    "                            for s_prime in xrange(N))\n",
    "    prob_O = sum(fwd[s][T-1] * to_end[s] for s in xrange(N))\n",
    "    print fwd\n",
    "    return prob_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02479     0.          0.          0.        ]\n",
      " [ 0.          0.00005303  0.          0.        ]\n",
      " [ 0.          0.          0.00000184  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "9.2140914902e-12\n"
     ]
    }
   ],
   "source": [
    "fwd = forward()\n",
    "print fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing Backward Probability $\\beta_t(j)$\n",
    "\n",
    "**meaning:** The probability of seeing the observations from time $t+1$ to the end, given that having been in state$j$ at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward(observation=O,HMM=LMD):\n",
    "    Q, A, B = HMM['Q'], HMM['A'], HMM['B']\n",
    "    from_start = HMM['from_start_probs']\n",
    "    to_end = HMM['to_end_probs']\n",
    "    O = observation\n",
    "    N, T = len(Q), len(O)\n",
    "    bwd = np.zeros((N,T))\n",
    "    for s in xrange(N): # 0=START -> N-1=END.\n",
    "        bwd[s][T-1] = to_end[s]\n",
    "    for t in reversed(xrange(T-1)): \n",
    "        for s_prime in xrange(N): # s_prime for previous-state relative to s.\n",
    "            bwd[s_prime][t] = sum(A[s_prime][s] * B[s][t+1] * bwd[s][t+1]\n",
    "                                  for s in xrange(N))\n",
    "    prob_O = sum(from_start[s]*B[s][0]*bwd[s][0] for s in xrange(N))\n",
    "    print bwd\n",
    "    return prob_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.00000147  0.001     ]\n",
      " [ 0.          0.00000017  0.00000351  0.05      ]\n",
      " [ 0.          0.          0.00000501  0.001     ]\n",
      " [ 0.          0.00000008  0.00000647  0.13      ]]\n",
      "9.2140914902e-12\n"
     ]
    }
   ],
   "source": [
    "bwd = backward()\n",
    "print bwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram HMM with FW-Algorithm (Single Observation)\n",
    "\n",
    "**NB: Multiple-Observation FW HMM training requires Lagrange Multiplier Optimization, which digresses somewhat and is not necessarily in understanding the FW-algorithm as a special case of EM algorithms. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_generator(len_seq=100):\n",
    "        # states = ['HOT','COLD']\n",
    "        # observations = [1,2,3]\n",
    "    rand = random.uniform(0,1)\n",
    "    start_state = 'HOT' if rand > .5 else 'COLD'\n",
    "    def transition(state):\n",
    "        rand = random.uniform(0,1)\n",
    "        if state=='HOT':  \n",
    "            return 'HOT' if rand < .7 else 'COLD'\n",
    "        else:\n",
    "            return 'HOT' if rand < .4 else 'COLD'\n",
    "        # transitions = np.array([[.7,.3],\n",
    "        #                         [.4,.6]])\n",
    "    def emission(state):\n",
    "        rand = random.uniform(0,1)\n",
    "        if state=='HOT':\n",
    "            if rand >= .6: return 3\n",
    "            elif rand < .2: return 1\n",
    "            else: return 2\n",
    "        else:\n",
    "            if rand >= .9: return 3\n",
    "            elif rand < .5: return 1\n",
    "            else: return 2\n",
    "        # emissions = np.array([[.2,.4,.4],\n",
    "        #                       [.5,.4,.1]])\n",
    "    tagged_sequence = []\n",
    "    prev_state = start_state\n",
    "    emitted = emission(prev_state)\n",
    "    for i in xrange(len_seq):\n",
    "        tagged_sequence.append((emitted,prev_state))\n",
    "        prev_state = transition(prev_state)\n",
    "        emitted = emission(prev_state)\n",
    "    observation_sequence = [o for o,s in tagged_sequence]\n",
    "    return tagged_sequence, observation_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwd_bwd_hmm(Q, O, test_O, num_iters=100): # notation: cf. J&M ch6.5:12-21.\n",
    "                                      # indexing for convenience in python: 1->N/T => 0->N-1/T-1.\n",
    "    N, T = len(Q), len(O)\n",
    "    # randomly initialize A (transitions) and B (emissions) matrices.\n",
    "    A = np.random.rand(N,N)\n",
    "    B = np.random.rand(N,T)\n",
    "    \n",
    "    from_start = to_end = 1/N # neutralize the effect from start->state & state->end.\n",
    "    # forward-backward probabilities computation functions.\n",
    "    #  notation: alpha_t(j)/beta_t(j) in J&M, but here we put rows for states,\n",
    "    #            instead of having, e.g., alpha[t][s] for easier visualization.\n",
    "    #            see previous forward/backward function to see why.\n",
    "    def forward():        \n",
    "        fwd = np.zeros((N,T))\n",
    "        for s in xrange(N):\n",
    "            fwd[s][0] = from_start * B[s][0]\n",
    "        for t in xrange(1,T):\n",
    "            for s in xrange(N):\n",
    "                fwd[s][t] = sum(fwd[s_prime][t-1] * A[s_prime][s] * B[s][t]\n",
    "                                for s_prime in xrange(N))\n",
    "        return fwd\n",
    "    def backward():\n",
    "        bwd = np.zeros((N,T))\n",
    "        for s in xrange(N): # 0=START -> N-1=END.\n",
    "            bwd[s][T-1] = to_end\n",
    "        for t in reversed(xrange(T-1)): \n",
    "            for s_prime in xrange(N): # s_prime for previous-state relative to s.\n",
    "                bwd[s_prime][t] = sum(A[s_prime][s] * B[s][t+1] * bwd[s][t+1]\n",
    "                                      for s in xrange(N))\n",
    "        return bwd\n",
    "    num_iter = 0\n",
    "    while num_iter < num_iters:\n",
    "        num_iter += 1\n",
    "        if num_iter%100==0: print \"... running the %dth EM iteration\" % num_iter\n",
    "        alpha = forward()\n",
    "        beta = backward()\n",
    "        prob_O = sum(alpha[s][T-1] for s in xrange(N)) * to_end \n",
    "            # alternatively: from_start * sum(B[s][0] * beta[s][0] for s in xrange(N))\n",
    "\n",
    "        # E-step\n",
    "        xi = np.zeros((T,N,N))\n",
    "        gamma = np.zeros((T,N))\n",
    "        for t in xrange(T-1):\n",
    "            for i in xrange(N):\n",
    "                for j in xrange(N):\n",
    "                    not_quite_xi = alpha[i][t]*A[i][j]*B[j][t+1]\n",
    "                    xi[t][i][j] = not_quite_xi / prob_O\n",
    "        for t in xrange(T):\n",
    "            for j in xrange(N):\n",
    "                gamma[t][j] = (alpha[j][t]*beta[j][t]) / prob_O\n",
    "\n",
    "        # M-step\n",
    "        for i in xrange(N):\n",
    "            for j in xrange(N):\n",
    "                A[i][j] = sum(xi[t][i][j] for t in xrange(T-1)) / \\\n",
    "                          sum(xi[t][i][k] for t in xrange(T-1) for k in xrange(N))\n",
    "                    # in the original algorithm, to_state is notated as j in\n",
    "                    #  both the numerator and the denominator, which can be confusing.\n",
    "        for j in xrange(N):\n",
    "            for k in xrange(T):\n",
    "                B[j][k] = sum(gamma[t][j]*B[j][k] for t in xrange(T)) / \\\n",
    "                          sum(gamma[t][j]*B[j][l] for t in xrange(T) for l in xrange(T))\n",
    "                    # k: current emission; l: variable for all possible emissions.\n",
    "    \n",
    "    N, T = len(Q), len(test_O) # some repetition for readers' convenience.\n",
    "    # viterbi\n",
    "    viterbi = np.zeros((N,T))\n",
    "    backpointer = np.zeros((N,T),dtype=int)\n",
    "    for s in range(N):\n",
    "        viterbi[s][0] = from_start * B[s][0]\n",
    "        backpointer[s][0] = 0\n",
    "    for t in xrange(1,T):\n",
    "        for s in xrange(N):\n",
    "            prev_viterbi = []\n",
    "            prev_backpointer = []\n",
    "            for s_prime in xrange(N):\n",
    "                prev_times_transition = viterbi[s_prime][t-1] * A[s_prime][s]\n",
    "                prev_viterbi.append(prev_times_transition * B[s][t])\n",
    "                prev_backpointer.append(prev_times_transition)\n",
    "            viterbi[s][t] = max(prev_viterbi)\n",
    "            backpointer[s][t] = np.argmax(prev_backpointer)\n",
    "    max_state = np.argmax(viterbi[:,-1])\n",
    "    best_tagged_sequence = []\n",
    "    for t in reversed(xrange(0,T)):\n",
    "        best_tagged_sequence.insert(0,(test_O[t],Q[max_state]))\n",
    "        max_state = backpointer[max_state][t]\n",
    "    \n",
    "    return A,B,best_tagged_sequence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DEMO\n",
    "\n",
    "** Caution: w/o the extension to Lagrange Multiplier Optimization, unsupervised learning with EM will not work in general. This is especially true when the number of state gets large. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_train, observed_train = random_generator(100)\n",
    "observed_test = observed_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'HOT'), (1, 'COLD'), (1, 'COLD'), (2, 'COLD'), (1, 'COLD'), (2, 'COLD'), (1, 'COLD'), (2, 'COLD'), (2, 'HOT'), (2, 'COLD'), (2, 'COLD'), (3, 'COLD'), (2, 'HOT'), (2, 'COLD'), (3, 'HOT'), (2, 'COLD'), (2, 'COLD'), (2, 'COLD'), (2, 'COLD'), (3, 'HOT'), (2, 'HOT'), (2, 'HOT'), (1, 'HOT'), (2, 'HOT'), (2, 'HOT'), (1, 'COLD'), (2, 'HOT'), (3, 'HOT'), (1, 'COLD'), (3, 'HOT'), (2, 'HOT'), (3, 'HOT'), (2, 'HOT'), (1, 'HOT'), (1, 'COLD'), (1, 'COLD'), (1, 'COLD'), (2, 'COLD'), (2, 'COLD'), (1, 'COLD'), (2, 'COLD'), (1, 'HOT'), (2, 'HOT'), (2, 'HOT'), (2, 'HOT'), (3, 'HOT'), (2, 'HOT'), (2, 'HOT'), (3, 'HOT'), (3, 'HOT'), (3, 'HOT'), (3, 'HOT'), (2, 'HOT'), (1, 'HOT'), (3, 'HOT'), (3, 'HOT'), (3, 'HOT'), (3, 'COLD'), (1, 'COLD'), (1, 'HOT'), (3, 'HOT'), (3, 'HOT'), (2, 'COLD'), (3, 'COLD'), (2, 'COLD'), (3, 'HOT'), (2, 'HOT'), (2, 'COLD'), (1, 'HOT'), (2, 'HOT'), (2, 'HOT'), (2, 'COLD'), (3, 'HOT'), (2, 'COLD'), (3, 'HOT'), (3, 'HOT'), (2, 'HOT'), (3, 'HOT'), (1, 'COLD'), (3, 'HOT'), (2, 'HOT'), (2, 'HOT'), (3, 'HOT'), (2, 'HOT'), (2, 'HOT'), (2, 'HOT'), (1, 'HOT'), (3, 'HOT'), (3, 'HOT'), (3, 'HOT'), (3, 'HOT'), (1, 'COLD'), (3, 'COLD'), (2, 'COLD'), (2, 'HOT'), (1, 'COLD'), (2, 'COLD'), (3, 'COLD'), (1, 'COLD'), (1, 'COLD')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 1, 2, 3, 1, 3, 2, 3, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 1, 3, 3, 3, 3, 1, 1, 3, 3, 2, 3, 2, 3, 2, 2, 1, 2, 2, 2, 3, 2, 3, 3, 2, 3, 1, 3, 2, 2, 3, 2, 2, 2, 1, 3, 3, 3, 3, 1, 3, 2, 2, 1, 2, 3, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print observed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 2, 1, 2, 1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print observed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... running the 10th EM iteration\n",
      "... running the 20th EM iteration\n",
      "... running the 30th EM iteration\n",
      "... running the 40th EM iteration\n",
      "... running the 50th EM iteration\n",
      "... running the 60th EM iteration\n",
      "... running the 70th EM iteration\n",
      "... running the 80th EM iteration\n",
      "... running the 90th EM iteration\n",
      "... running the 100th EM iteration\n",
      "CPU times: user 2min 14s, sys: 680 ms, total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "A,B,predicted_sequence = fwd_bwd_hmm(['HOT','COLD'],observed_train,observed_test,num_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'HOT'), (1, 'COLD'), (1, 'COLD'), (2, 'COLD'), (1, 'COLD'), (2, 'COLD'), (1, 'COLD'), (2, 'COLD'), (2, 'COLD'), (2, 'COLD')]\n"
     ]
    }
   ],
   "source": [
    "print predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'HOT'), (1, 'COLD'), (1, 'COLD'), (2, 'COLD'), (1, 'COLD'), (2, 'COLD'), (1, 'COLD'), (2, 'COLD'), (2, 'HOT'), (2, 'COLD')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Algorithm\n",
    "\n",
    "** EM tutorial by Francisco Iacobelli; Code by Jacob Su Wang **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coin(theta=.5, num=100):\n",
    "    return ['H' if random.uniform(0,1)<theta else 'T' for _ in xrange(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biased_coin = coin(theta=.8)\n",
    "fair_coin = coin(theta=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'H': 84, 'T': 16})\n",
      "Counter({'H': 52, 'T': 48})\n"
     ]
    }
   ],
   "source": [
    "print Counter(biased_coin)\n",
    "print Counter(fair_coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
