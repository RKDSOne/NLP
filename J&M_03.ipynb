{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** HMM Formalism ($HMM\\space\\lambda = (A,B)$) ** (cf. J&M ch5.5:23)\n",
    "\n",
    "    * $Q = q_1...q_N$, a set of $N$ *states*.\n",
    "    * $A = a_{11}a_{12}...a_{nn}$, where $\\sum_{j=1}^na_{ij} = 1, \\forall i$, a *transition probability matrix* $A$.\n",
    "    * $O = o_1...o_T$, a sequence of *observations*.\n",
    "    * $B = b_i(o_t)$, a sequence of *emission probabilities* (i.e. the probability of an observation $o_t$ being omitted at state $i$.\n",
    "    * $q_0,q_F$, *start state* and *end state*.\n",
    "\n",
    "\n",
    "* ** Framing Problems ** (cf. J&M ch6.2:7)\n",
    "\n",
    "    * ** Problem 1 (Computing Likelihood): ** Given an HMM $\\lambda=(A,B)$ and an observation sequence $O$, determine the likelihood $P(O|\\lambda)$.\n",
    "\n",
    "    * ** Problem 2 (Decoding): ** Given an observation sequence $O$ and an HMM $\\lambda=(A,B)$, discover the best hidden state sequence $Q$.\n",
    "\n",
    "    * ** Problem 3 (Learning): ** Given an observation sequence $O$ and the set of states in the HMM, learn the HMM parameters $A$ and $B$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Computing Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ICE CREAM MODEL\n",
    " \n",
    "#  -.8-> HOT -.7-> HOT -.3-> COLD\n",
    "#         |         |         |\n",
    "#        .4        .2        .1\n",
    "#         3         1         3\n",
    "\n",
    "# Question: P(O = [3,1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Math ** (cf. J&M ch6.3:9,eq.6.10,12)\n",
    "\n",
    "    * $P(O,Q) = P(O|Q)\\cdot P(Q) = \\prod_{i=1}^nP(o_i|q_i)\\times\\prod_{i=1}^nP(q_i|q_{i-1})$, where $n$ is the ordered index sequence of observations.\n",
    "    \n",
    "    * $P(O) = \\sum_QP(O,Q) = \\sum_QP(O|Q)\\cdot P(Q)$\n",
    "    \n",
    "\n",
    "* ** Computation ** (cf. ibid.:10-12)\n",
    "\n",
    "    * Brute-Force: $O(N^T)$, where $N$ is the set of tags, $T$ the length of observation sequence.\n",
    "    \n",
    "    * Forward Algorithm: $O(N^2T)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TOY HMM\n",
    "class LMD:\n",
    "    def __init__(self):\n",
    "        self.Q = ['START','COLD','HOT'] # NB: the order matters for indexing reason later.\n",
    "        self.A = {('START','START'):0.,('START','HOT'):.8,('START','COLD'):.2,\n",
    "                  ('HOT','START'):0.,('HOT','HOT'):.7,('HOT','COLD'):.3,\n",
    "                  ('COLD','START'):0.,('COLD','HOT'):.4,('COLD','COLD'):.6}\n",
    "        self.B = {('HOT',1):.2,('HOT',2):.4,('HOT',3):.4,\n",
    "                  ('COLD',1):.5,('COLD',2):.4,('COLD',3):.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute-Force \"Baseline\"\n",
    "\n",
    "* num_states | processing time:\n",
    "    * 3 | 300 $\\mu$s\n",
    "    * 15 | 800 ms\n",
    "    * 20 | 30 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from nltk.util import ngrams\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brute_force(lmd,O,num_states=3):\n",
    "    Q, A, B = lmd.Q, lmd.A, lmd.B\n",
    "    Qs = [list(Q) for Q in product(Q[1:],repeat=num_states)] # all possible t-state sequences.\n",
    "    # computation components\n",
    "    def Pr_Q(Q): # Q = [q_1,...,q_n].\n",
    "        Q = deepcopy(Q)\n",
    "        Q.insert(0,'START')\n",
    "        p = 1.\n",
    "        for q_trans in ngrams(Q,2): # q_trans = (q_i-1,q_i).\n",
    "            p *= A[q_trans]\n",
    "        return p\n",
    "    def Pr_O_given_Q(O,Q): # O = [o_1,...,o_t], Q = [q_1,...,q_n].\n",
    "        return reduce(mul,map(B.get,zip(Q,O)))\n",
    "    def Pr_O(O):\n",
    "        return sum(Pr_O_given_Q(O,Q)*Pr_Q(Q) for Q in Qs)      \n",
    "    print Pr_O(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026264\n",
      "CPU times: user 31.8 s, sys: 230 ms, total: 32 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lmd = LMD()\n",
    "O = [3,1,3]\n",
    "brute_force(lmd,O,num_states=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Algorithm\n",
    "\n",
    "* num_states | processing time:\n",
    "    * 3 | 300 $\\mu$s\n",
    "    * 15 | 400 $\\mu$s\n",
    "    * 20 | 500 $\\mu$s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(lmd,O):\n",
    "    Q, A, B = lmd.Q, lmd.A, lmd.B\n",
    "    N, T = len(Q), len(O)\n",
    "    fwd = np.zeros((N,T))\n",
    "    for s in xrange(1,N):\n",
    "        fwd[s][1] = A[('START',Q[s])] * B[(Q[s],O[1])]\n",
    "            # initialize: START -> all states\n",
    "    for t in xrange(2,T):\n",
    "        for s in xrange(1,N):\n",
    "            fwd[s][t] = sum(fwd[s_prime][t-1]*A[(Q[s_prime],Q[s])]*B[(Q[s],O[t])]\n",
    "                            for s_prime in xrange(1,N)) \n",
    "            # forward passing: sum ( each prev state * prev->current * current->omission )\n",
    "    print 'Pr(O) = ', sum(fwd[:,-1])\n",
    "    \n",
    "# fwd matrix in when O = [3,1,3], (cf. J&M ch6.4:10,fig.6.7)\n",
    "# [[ 0.        0.        0.        0.      ]\n",
    "#  [ 0.        0.02      0.054     0.004632]\n",
    "#  [ 0.        0.32      0.0464    0.021632]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(O) =  0.026264\n",
      "CPU times: user 271 µs, sys: 94 µs, total: 365 µs\n",
      "Wall time: 293 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O = ['<s>',3,1,3]\n",
    "forward(LMD(),O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Decoding: Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TOY HMM again\n",
    "class LMD:\n",
    "    def __init__(self):\n",
    "        self.Q = ['START','COLD','HOT'] # NB: the order matters for indexing reason later.\n",
    "        self.A = {('START','START'):0.,('START','HOT'):.8,('START','COLD'):.2,\n",
    "                  ('HOT','START'):0.,('HOT','HOT'):.7,('HOT','COLD'):.3,\n",
    "                  ('COLD','START'):0.,('COLD','HOT'):.4,('COLD','COLD'):.6}\n",
    "        self.B = {('HOT',1):.2,('HOT',2):.4,('HOT',3):.4,\n",
    "                  ('COLD',1):.5,('COLD',2):.4,('COLD',3):.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Math (Recursion Step) **\n",
    "\n",
    "    * $viterbi_{s,t} = max(viterbi_{s',t-1} * A_{s',s} * B_{s,t})$, i.e. at each time $t$, at state $s$, set viterbi as **the largest value** of the multiplication $\\mathtt{value\\,of\\,prev\\,state}\\times\\mathtt{transition\\,from\\,prev\\,to\\,current}\\times\\mathtt{current\\,emission}$.\n",
    "    \n",
    "    * $backpointer_{s,t} = argmax(viterbi_{s',t-1} * A_{s',s})$, i.e. at each time $t$, at state $s$, set backpointer as the **index of largest value** of the multiplication $\\mathtt{value\\,of\\,prev\\,state}\\times\\mathtt{transition\\,from\\,prev\\,to\\,current}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(lmd,O):\n",
    "    Q, A, B = lmd.Q, lmd.A, lmd.B\n",
    "    N, T = len(Q), len(O)\n",
    "    viterbi, backpointer = np.zeros((N,T)), np.zeros((N,T),dtype=int)\n",
    "    for s in xrange(1,N): \n",
    "        # this is written in a more transparent syntax, for simplification, see J&M_02.\n",
    "        viterbi[s][1] = A[('START',Q[s])] * B[(Q[s],O[1])]\n",
    "        backpointer[s][1] = 0\n",
    "    for t in xrange(2,T):\n",
    "        for s in xrange(1,N):\n",
    "            viterbi[s][t] = max(viterbi[s_prime][t-1]*A[(Q[s_prime],Q[s])]*B[(Q[s],O[t])]\n",
    "                                for s_prime in xrange(1,N))\n",
    "            backpointer[s][t] = np.argmax([viterbi[s_prime][t-1]*A[(Q[s_prime],Q[s])]\n",
    "                                           for s_prime in xrange(1,N)])+1\n",
    "    max_state = np.argmax(viterbi[:,-1])\n",
    "    best_state_seq = []\n",
    "    for t in reversed(xrange(0,T)): \n",
    "        best_state_seq.insert(0,Q[max_state])\n",
    "        max_state = backpointer[max_state][t]\n",
    "    \n",
    "    print 'viterbi'\n",
    "    print viterbi\n",
    "    print 'backpointer'\n",
    "    print backpointer\n",
    "    print best_state_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbi\n",
      "[[ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.02      0.048     0.00288 ]\n",
      " [ 0.        0.32      0.0448    0.012544]]\n",
      "backpointer\n",
      "[[0 0 0 0]\n",
      " [0 0 2 1]\n",
      " [0 0 2 2]]\n",
      "['START', 'HOT', 'HOT', 'HOT']\n",
      "CPU times: user 1.48 ms, sys: 531 µs, total: 2.01 ms\n",
      "Wall time: 1.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "O = ['<s>',3,1,3]\n",
    "viterbi(LMD(),O) \n",
    "    # NB: J&M ch6.4:13,fig.6.10 is not showing the best tag sequence for O=[3,1,3],\n",
    "    #  specifically, the figure is inconsistent with the transition/emission probabilities\n",
    "    #  the example is working with (e.g. the numbers in LMD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trigram HMM + Deleted Interpolation (Brown Corpus)\n",
    "\n",
    "**Comments: Somewhat disapointing, it doesn't outperform the previous bigram HMM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_brown(train_percentage=.8): # importing tagged brown.\n",
    "    from nltk.corpus import brown\n",
    "    from nltk.stem import PorterStemmer\n",
    "    print \"... loading sentences\"\n",
    "    brown_tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "    print \"... stemming and lowercasing words\"\n",
    "    brown_tagged_sents = [[(PorterStemmer().stem(w).lower(),t) for w,t in tagged_sent]\n",
    "                   for tagged_sent in brown_tagged_sents]\n",
    "    print \"... padding sentences\"\n",
    "    brown_tagged_sents = [[('<s>','START')]+s+[('</s>','END')] for s in brown_tagged_sents]\n",
    "    cut_off = int(len(brown_tagged_sents)*.8)\n",
    "    \n",
    "    return (brown_tagged_sents[:cut_off], brown_tagged_sents[cut_off:]) # train,test: two lists of sents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading sentences\n",
      "... stemming and lowercasing words\n",
      "... padding sentences\n",
      "CPU times: user 22.7 s, sys: 330 ms, total: 23.1 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "brown_tagged_train, brown_tagged_test = load_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from numpy import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        print \"... setting up vocabularies for tags and words\"\n",
    "        self.size = sum(map(len,self.train))\n",
    "        self.vocab = list({w for sent in self.train for w,t in sent}) + ['unk'] # brown: len=31661, as T.\n",
    "        self.tagset = list({t for sent in self.train for w,t in sent}) + ['UNK'] # brown: len=452, as N.\n",
    "        self.btagset = Counter(ngrams([t for sent in self.train for w,t in sent],2)).keys()\n",
    "        self.w2i = Counter({w:i for i,w in enumerate(self.vocab)})\n",
    "        self.t2i = Counter({t:i for i,t in enumerate(self.tagset)})\n",
    "        self.bt2i = Counter({bt:i for i,bt in enumerate(self.btagset)})\n",
    "        print \"... building transition probability matrix\"\n",
    "        self.build_transition_matrix()\n",
    "        print \"... building emission probability matrix\"\n",
    "        self.build_emission_matrix()\n",
    "    \n",
    "    def build_transition_matrix(self):\n",
    "        # Comments: The following can be improved my cramping all\n",
    "        #  three ngram matrices into one, however, I suspect the \n",
    "        #  gigantic matrix resulted from that will cause some\n",
    "        #  computational difficulties.\n",
    "        \n",
    "        print \"    ... computing unigram probabilities\"\n",
    "        unigram_dict = Counter(t for sent in self.train for w,t in sent)\n",
    "        self.A_uni = np.ones(len(self.tagset))\n",
    "        for t in self.tagset:\n",
    "            count = unigram_dict[t]\n",
    "            self.A_uni[self.t2i[t]] = count if count!=0 else 1.\n",
    "        row_sum = sum(self.A_uni)\n",
    "        self.A_uni /= row_sum\n",
    "        self.A_uni[isnan(self.A_uni)] = 0.\n",
    "        \n",
    "        print \"    ... computing bigram transition probabilities\"\n",
    "        bi_trans_dict = Counter(ngrams([t for sent in self.train for w,t in sent],2))\n",
    "        self.A_bi = np.ones((len(self.tagset),len(self.tagset))) \n",
    "        for from_t in self.tagset:\n",
    "            for to_t in self.tagset:\n",
    "                trans_count = bi_trans_dict[(from_t,to_t)]\n",
    "                self.A_bi[self.t2i[from_t]][self.t2i[to_t]] = trans_count if trans_count!=0 else 1.\n",
    "        row_sums = np.apply_along_axis(sum, 1, self.A_bi)[:,np.newaxis]\n",
    "        self.A_bi /= row_sums\n",
    "        self.A_bi[isnan(self.A_bi)] = 0.\n",
    "        \n",
    "        print \"    ... computing trigram transition probabilities\"\n",
    "        tri_trans_dict = Counter(ngrams([t for sent in self.train for w,t in sent],3))\n",
    "        self.A_tri = np.ones((len(self.btagset),len(self.tagset)))\n",
    "        for from_t2,from_t1 in self.btagset: # i.e. t_{i-2}, t_{i-1}\n",
    "            for to_t in self.tagset:\n",
    "                trans_count = tri_trans_dict[(from_t2,from_t1,to_t)]\n",
    "                self.A_tri[self.bt2i[(from_t2,from_t1)]][self.t2i[to_t]] = trans_count if trans_count!=0 else 1.\n",
    "        row_sums = np.apply_along_axis(sum, 1, self.A_tri)[:,np.newaxis]\n",
    "        self.A_tri /= row_sums\n",
    "        self.A_tri[isnan(self.A_tri)] = 0.\n",
    "        \n",
    "        print \"    ... computing deleted interpolation coefficients\"\n",
    "        self.lmd1, self.lmd2, self.lmd3 = self.deleted_interpolation(unigram_dict,bi_trans_dict,tri_trans_dict)\n",
    "    \n",
    "    def build_emission_matrix(self):\n",
    "        print \"    ... counting emissions\"\n",
    "        emission_dict = Counter((w,t) for sent in self.train for w,t in sent)\n",
    "        print \"    ... computing emission probabilities\"\n",
    "        self.B = np.ones((len(self.tagset),len(self.vocab)))\n",
    "        for t in self.tagset:\n",
    "            for o in self.vocab:\n",
    "                emit_count = emission_dict[(o,t)]\n",
    "                self.B[self.t2i[t]][self.w2i[o]] = emit_count if emit_count!=0 else 1.\n",
    "        row_sums = np.apply_along_axis(sum, 1, self.B)[:,np.newaxis]\n",
    "        self.B /= row_sums\n",
    "        self.B[isnan(self.B)] = 0.\n",
    "    \n",
    "    def deleted_interpolation(self, unigrams, bigrams, trigrams):\n",
    "        # unigrams, bigrams, trigrams: 3 Counters.\n",
    "        def brants_divide(num,denom): # (cf. Brants(2000):226,fig.1)\n",
    "            return 0 if denom==0 else num/denom\n",
    "        lmds = {1:0, 2:0, 3:0}\n",
    "        for t1,t2,t3 in trigrams:\n",
    "            f123 = trigrams[(t1,t2,t3)] / bigrams[(t1,t2)]\n",
    "            lmds[np.argmax([-np.inf,\n",
    "                            brants_divide(trigrams[(t1,t2,t3)]-1,bigrams[(t1,t2)]-1),\n",
    "                            brants_divide(bigrams[(t2,t3)]-1,unigrams[t3]-1),\n",
    "                            brants_divide(unigrams[t3]-1,self.size-1)])] += f123\n",
    "        return np.array([lmds[1],lmds[2],lmds[3]]) / sum(np.array([lmds[1],lmds[2],lmds[3]]))\n",
    "\n",
    "    def viterbi(self, sent):\n",
    "        # sent: a sentence in the form, e.g. ['<s>',w1,...,wN,'</s>'].\n",
    "        \n",
    "        N, T = len(self.tagset), len(sent)\n",
    "        viterbi = np.zeros((N,T))\n",
    "        backpointer = np.zeros((N,T),dtype=int)\n",
    "        \n",
    "        for s in xrange(N):\n",
    "            viterbi[s][1] = self.A_bi[self.t2i['START']][s] * self.B[s][self.w2i[sent[1]]]\n",
    "            backpointer[s][1] = 0            \n",
    "            \n",
    "        for t in xrange(2,T):\n",
    "            for s in xrange(N):\n",
    "                prev_viterbi = []\n",
    "                prev_backpointer = []\n",
    "                for s_prime in xrange(N):\n",
    "                    max_trans = max(self.lmd1*self.A_uni[s] + \\\n",
    "                                    self.lmd2*self.A_bi[s_prime][s] + \\\n",
    "                                    self.lmd3*self.A_tri[self.bt2i[(self.tagset[s_prime_prime],\n",
    "                                                                    self.tagset[s_prime])]][s]\n",
    "                                    for s_prime_prime in xrange(N))\n",
    "                    prev_viterbi.append(viterbi[s_prime][t-1] * max_trans * self.B[s][self.w2i[sent[t]]])\n",
    "                    prev_backpointer.append(viterbi[s_prime][t-1] * max_trans)\n",
    "                viterbi[s][t] = max(prev_viterbi)\n",
    "                backpointer[s][t] = np.argmax(prev_backpointer)  \n",
    "                     \n",
    "        max_state = self.tagset.index('END') \n",
    "            # TEST: always give </s> END as tag.\n",
    "            #  otherwise: max_state = np.argmax(viterbi[:,T-1])\n",
    "        best_tagged_seq = []\n",
    "        for t in reversed(xrange(0,T)):\n",
    "            best_tagged_seq.insert(0,(sent[t],self.tagset[max_state]))\n",
    "            max_state = backpointer[max_state][t]\n",
    "        \n",
    "        return best_tagged_seq\n",
    "    \n",
    "    def tag(self, sent): # may use this for other passing purpose.\n",
    "        return self.viterbi(sent) \n",
    "\n",
    "    def evaluate(self, k=500, verbose_freq=100):\n",
    "        sample_idxs = random.sample(xrange(len(self.test)),k)\n",
    "        sample_test = [self.test[i] for i in sample_idxs] \n",
    "        accuracies = []\n",
    "        sent_lens = []\n",
    "        for i in xrange(k):\n",
    "            sent = [w if w in self.vocab else 'unk' for w,t in sample_test[i]]\n",
    "            tagged = self.tag(sent)\n",
    "            accuracy = sum(y==yhat for y,yhat in zip(sample_test[i][1:-1],tagged[1:-1])) / \\\n",
    "                                                                  len(tagged[1:-1])\n",
    "            if i!=0 and i%verbose_freq==0:\n",
    "                print \"    ... tagged %d sentences, current accuracy: %.2f%%\" % (i,np.mean(accuracies)*100) \n",
    "            accuracies += [accuracy]\n",
    "            sent_lens += [len(tagged[1:-1])] \n",
    "            \n",
    "        return [sent_lens, accuracies]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... setting up vocabularies for tags and words\n",
      "... building transition probability matrix\n",
      "    ... computing unigram probabilities\n",
      "    ... computing bigram transition probabilities\n",
      "    ... computing trigram transition probabilities\n",
      "    ... computing deleted interpolation coefficients\n",
      "... building emission probability matrix\n",
      "    ... counting emissions\n",
      "    ... computing emission probabilities\n",
      "CPU times: user 5.54 s, sys: 182 ms, total: 5.72 s\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hmm = HMM(brown_tagged_train,brown_tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ... tagged 100 sentences, current accuracy: 90.21%\n",
      "    ... tagged 200 sentences, current accuracy: 90.74%\n",
      "    ... tagged 300 sentences, current accuracy: 90.28%\n",
      "    ... tagged 400 sentences, current accuracy: 89.89%\n",
      "    ... tagged 500 sentences, current accuracy: 90.12%\n",
      "    ... tagged 600 sentences, current accuracy: 89.99%\n",
      "    ... tagged 700 sentences, current accuracy: 90.16%\n",
      "    ... tagged 800 sentences, current accuracy: 90.25%\n",
      "    ... tagged 900 sentences, current accuracy: 90.35%\n",
      "    ... tagged 1000 sentences, current accuracy: 90.16%\n",
      "    ... tagged 1100 sentences, current accuracy: 90.19%\n",
      "    ... tagged 1200 sentences, current accuracy: 90.22%\n",
      "    ... tagged 1300 sentences, current accuracy: 90.23%\n",
      "    ... tagged 1400 sentences, current accuracy: 90.28%\n",
      "    ... tagged 1500 sentences, current accuracy: 90.29%\n",
      "    ... tagged 1600 sentences, current accuracy: 90.24%\n",
      "    ... tagged 1700 sentences, current accuracy: 90.27%\n",
      "    ... tagged 1800 sentences, current accuracy: 90.26%\n",
      "    ... tagged 1900 sentences, current accuracy: 90.22%\n",
      "CPU times: user 4min 39s, sys: 1.19 s, total: 4min 40s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sent_lens, accuracies = hmm.evaluate(k=2000,verbose_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Learning: Forward-Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TOY HMM\n",
    "LMD = {'Q':['START','PPSS','VB','TO','NN', 'END'], \n",
    "       'A': np.array([[0.,    .067,    .019,    .0043,    .041,    0.],    # rows: from-states, cols: to-states.\n",
    "                      [0.,    .00013, .23,     .00079,   .0012,   .001],  # rows=cols={START,PPSS,VB,TO,NN,END}, len=6\n",
    "                      [0.,    .007,   .0038,   .035,     .047,    .05],   # e.g. A[4][2] = p(TO->VB) = .83.\n",
    "                      [0.,    0.,     .83,     0.,       .00047,  .00001],  \n",
    "                      [0.,    .0045,  .004,    .016,     .087,    .13],\n",
    "                      [0.,    0.,     0.,      0.,       0.,      0.]]),\n",
    "       'B': np.array([[1.,     0.,     0.,     0.,     0.,     0.], # rows: emitting-state, cols: emitted-observations.\n",
    "                      [0.,     .37,    0.,     0.,     0.,     0.], #  rows={START,PPSS,VB,TO,NN,END}, len=6\n",
    "                      [0.,     0.,     .0093,  0.,     .00012, 0.], #  cols={<s>,i,want,to,race,</s>}, len=6  \n",
    "                      [0.,     0.,     0.,     .99,    0.,     0.], # e.g. B[2][2] = p(VB->want) = .37. \n",
    "                      [0.,     0.,     .000054,0.,     .00057, 0.],\n",
    "                      [0.,     0.,     0.,     0.,     0.,     1.]])}\n",
    "\n",
    "O = ['<s>','i','want','to','race','</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing Forward Probability $\\alpha_t(j)$ (state=j at time=t), given HMM $\\lambda=\\{A,B\\}$ and Observation $O$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(t_,j_,observation=O,HMM=LMD):\n",
    "    Q, A, B = HMM['Q'], HMM['A'], HMM['B']\n",
    "    O = observation\n",
    "    N, T = len(Q), len(O)\n",
    "    fwd = np.zeros((N,T))\n",
    "    if t_==1: return A[0][j_] * B[j_][t_]\n",
    "    for s in xrange(1,N):\n",
    "        fwd[s][1] = A[0][s] * B[s][1]\n",
    "    for t in xrange(2,t_):\n",
    "        for s in xrange(1,N):\n",
    "            fwd[s][t] = sum(fwd[s_prime][t-1] * A[s_prime][s] * B[s][t]\n",
    "                            for s_prime in xrange(1,N))\n",
    "    print fwd\n",
    "    return sum(fwd[s_prime][t_-1] * A[s_prime][j_] for s_prime in xrange(1,N)) * B[j_][t_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.       0.       0.       0.       0.       0.     ]\n",
      " [ 0.       0.02479  0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.       0.       0.     ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.3025810000000004e-05"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(2,2) # state=2 at time=2 (i.e. VB at 'want')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
